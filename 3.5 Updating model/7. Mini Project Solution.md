Loading and Testing the New Data
---

[watch video](https://www.youtube.com/watch?v=75RxW3R6674)

* collect new data
    * encode new data to send to model

* check how well the model is performing on new data
    * save the new transformed data, upload to S3.
    * set transform job
    * check the results

* reflects on the new results

Exploring the New Data
---
[watch video](https://www.youtube.com/watch?v=sEBK1dmiUfE)

* Assume the model is not doing well on the new data
    * iterate over the misclassified sample -- use generator using 'yield' keyword
    * In the sentiment analysis example, we recreate our 5000 frequency words vocabulary by using the new data
    * look at the difference between the old and new vocabulary, figure out how frequent do the discrepancies occur in each vocabulary.
    ```
    print(original_vocab - new_vocab)
    >>> {'21st', 'playboy', 'reincarn', 'victorian', 'ghetto', 'weari'} ##the words that do not exist in the new vocabulary
    print(new_vocab - original_vocab)
    >>> {'dubiou', 'sophi', 'banana', 'omin', 'orchestr', 'materson', 'optimist'} ##the words that do not exist in the old vocabulary
    ```


Building a New Model
---

[watch video](https://www.youtube.com/watch?v=RUVxrKcWAsU)

* In our sentiment analysis example, we build a new model fitting our new data, which is split into train, validation and test set
* test the new model on the new data: create transform job on new data.
* check the result
* now, check the new model on the old data. We want to make sure the new model works well on the old data as well.
* crate transform job
* check result
* if the result is meeting expectation, update the endpoint, so the flow of new data will use the updated model you just created. To do so,
* create a new endpoint configuration by updating the new model name, which can be extracted from the transformer object attribute `.model_name` you created in prior steps
* update sagemaker endpoint by `update_endpoint()` method
